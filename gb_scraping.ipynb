{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YunhoCha/2024-DH/blob/main/gb_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouaAqullifrx",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "# 1_웹 스크래핑"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습 목표\n",
        "- 국사편찬위원회 한국사데이터베이스의 사료를 자유롭게 크롤링하고 수정하여 나만의 DB를 만들 수 있다.\n",
        "- 형태소분석기를 활용해 사료 텍스트에서 개념어를 추출하고 빈도 수를 분석해본다.\n",
        "- 위 과정에 활용되는 프로그래밍 언어의 작동 원리를 이해하고, chatGPT라는 보조도구를 활용해 평소에도 써본다.\n",
        "\n",
        "**주의**\n",
        "- 시작전 왼쪽에 data 폴더와 result 폴더을 만들고 data 폴더에 메타데이터 파일을 꼭 넣을 것"
      ],
      "metadata": {
        "id": "Sg5xvXi0GFWC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5I4gxbtiklW"
      },
      "source": [
        "# 2_필요한 도구와 라이브러리 설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQEi61fV8eIV",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import ssl\n",
        "import pandas as pd\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBIFV5Lrir6Y"
      },
      "source": [
        "# 3_url 분석"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEzhAIFS8eUq",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# \"개벽\" 기사가 서비스되는 웹페이지 주소 1개를 불러와서 url 변수에 저장하기\n",
        "# 이 url에서 기사별 id가 있는 부분을 경계로 그 이전과 이후를 분리하기\n",
        "\n",
        "url = 'https://db.history.go.kr/modern/level.do?levelId=ma_013_0010_0001'\n",
        "url_s = url.split('ma_013_0010_0001')\n",
        "print(url)\n",
        "print(url_s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4fc4WjgjAwo"
      },
      "source": [
        "# 4_기사정보 다운로드"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9fdy_59jIIY"
      },
      "source": [
        "사전 작업\n",
        "- 국편 한국사데이터베이스/개벽에서 기사정보 다운로드\n",
        "- '개벽_기사정보.txt'로 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPc7GRf4dXSv",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# 이름 조심하기\n",
        "gb_info = 'data/근현대잡지자료 수정본.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfFVBJ4XdlhN",
        "tags": []
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(gb_info, delimiter='^', encoding='utf-8')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUfc8PDzEnyg",
        "tags": []
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kg2nv7iEyvE",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# '발행일' 열이 공백이면 기사내용이 포함되지 않은 행이라는 뜻\n",
        "df1 = df[['자료ID', '기사제목', '필자', '기사형태', '발행일', 'URL']]\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "j1ypiBfwDRVJ"
      },
      "outputs": [],
      "source": [
        "# 결측치 확인, 발행일열이 공백인 77개의 행을 없앨 필요가 있음\n",
        "df1.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "85esmGBCDRVJ"
      },
      "outputs": [],
      "source": [
        "# 발행일 열이 공백인 행을 보기\n",
        "df1[df1['발행일'].isna()][:30]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "7O5BLuAtDRVJ"
      },
      "outputs": [],
      "source": [
        "# 요약통계\n",
        "df1.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "TxFRvzxaDRVJ"
      },
      "outputs": [],
      "source": [
        "# 값 파악\n",
        "df1['발행일'].value_counts().sort_index()[60:90]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "6t6Ob-OVDRVJ"
      },
      "outputs": [],
      "source": [
        "# 결측치 제거\n",
        "df1 = df1.copy()\n",
        "df1 = df1.dropna(subset = ['발행일'])  # 발행일 열이 비어있는 곳을 없애기\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "bqZ9Sfg1DRVJ"
      },
      "outputs": [],
      "source": [
        "# 1930년대 개벽 제외\n",
        "df1['year'] = df1['발행일'].apply(lambda x: int(x[:4]))\n",
        "df2 = df1.query('year < 1930')\n",
        "df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "_kq6vyEUDRVJ"
      },
      "outputs": [],
      "source": [
        "df2.info() # 자료id가 2257개 임을 알 수 있음"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "GT0ZD5tiDRVJ"
      },
      "outputs": [],
      "source": [
        "df2.isna().sum() # 다시한번 결측치를 확인하니 발행일 쪽 결측치가 0이 됨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "q7Gz3oLDDRVK"
      },
      "outputs": [],
      "source": [
        "# 자료ID 길이 확인 <- 크로스 체크\n",
        "df2['자료ID'].str.len().value_counts()\n",
        "# 자료id가 글자길이가 16인 것이 2257개임을 알 수 있음\n",
        "# 만약 글자길이가 다른 id가 존재한다면 그것도 결측치로 제거할 수 있음(밑 코드 이용)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df2 = df1[df1['자료ID'].str.len() != 17] # 길이가 17인 자료id 행 삭제\n",
        "# df2"
      ],
      "metadata": {
        "id": "GBunR5RQNJmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXPq0wwTmvip"
      },
      "source": [
        "# 5_기사 id를 이용한 원문 스크래핑\n",
        "- url_s 변수를 이용하여 기사 ID를 포함한 웹페이지 주소를 생성\n",
        "- 이 주소를 이용하여 웹 페이지에 접속하고, BeautifulSoup을 사용하여 원하는 부분의 데이터를 추출\n",
        "- 국편위 개별 사료 페이지로 가서 Ctrl + Shift + i 를 누르기, html에서 텍스트가 어느 위치에 있는지 알아보기(실습)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYorz5nThzuN",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# 기사 id를 모두 리스트로 변환\n",
        "\n",
        "id_List = df2['자료ID'].to_list()\n",
        "print(len(id_List))\n",
        "id_List[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcAwkgIKAjcL",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# 본문 가져오기 사용자 함수 작성\n",
        "\n",
        "def get_contents(url):\n",
        "    nm = 1\n",
        "    results = []  # 결과를 저장할 리스트 생성\n",
        "\n",
        "    # SSL 인증서 확인을 건너뛰도록 설정\n",
        "    ctx = ssl.create_default_context()\n",
        "    ctx.check_hostname = False\n",
        "    ctx.verify_mode = ssl.CERT_NONE\n",
        "\n",
        "    for ids in id_List[:5]: # 샘플로 5개 기사만 추출, 숫자를 아까 자료 id 전체 크기로 바꾸면 모든 사료를 스크래핑하는 것\n",
        "        N_url = url_s[0] + ids + url_s[-1]\n",
        "        # SSL 인증서 확인을 건너뛰는 urlopen 사용\n",
        "        webpage = urlopen(N_url, context=ctx)\n",
        "\n",
        "        bsobj = BeautifulSoup(webpage.read(), 'html.parser')\n",
        "        List1 = bsobj.findAll('div', {'id': 'cont_view'})  # 이 부분을 국편위 사이트 개별 사료에서 html을 파악하여 위치를 넣는 곳\n",
        "\n",
        "        for z in List1:\n",
        "            z1 = z.get_text('\\n', strip=True)\n",
        "            results.append([ids, z1])  # 결과를 리스트에 추가\n",
        "\n",
        "    # pandas DataFrame으로 변환\n",
        "    result_df = pd.DataFrame(results, columns=['자료ID', 'Content'])\n",
        "    return result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ty_FfQ1hpTJX",
        "tags": []
      },
      "outputs": [],
      "source": [
        "contents_5_df = get_contents(url)\n",
        "contents_5_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qi38QdJ5IjAc",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# 본문 내용 확인 (첫 행)\n",
        "print(contents_5_df.loc[3,'Content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbLnDXixpwQ_"
      },
      "source": [
        "# 6_데이터 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_QREybe1CcG",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# 두 데이터프레임을 '자료ID' 기준으로 통합\n",
        "\n",
        "combi_df = pd.merge(df2, contents_5_df, on='자료ID', how='inner')\n",
        "combi_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5Ta24W21nO2",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# 저장\n",
        "combi_df.to_excel('result/A1_raw5.xlsx', index=False)  # 인덱스를 저장하지 않도록 설정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtsnJaro1nL-"
      },
      "source": [
        "# The End of Note"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "XPZIoTXNOZsM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 텍스트 전처리(형태소 분석)과 간단한 빈도 분석 실습"
      ],
      "metadata": {
        "id": "RqkPKkzV60p4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 형태소 분석(Kiwi 형태소 분석기)"
      ],
      "metadata": {
        "id": "EJSduDGZO5Wc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) 데이터 불러오기"
      ],
      "metadata": {
        "id": "xV4mhqo_PDXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import re"
      ],
      "metadata": {
        "id": "oYIQVR3F67q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 설정된 경로에서 엑셀 파일 불러오기\n",
        "rawdata = 'result/A1_raw5.xlsx'"
      ],
      "metadata": {
        "id": "CbCPSu9rPGhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 불러오기\n",
        "\n",
        "df = pd.read_excel(rawdata)\n",
        "print(df.shape)\n",
        "\n",
        "df1 = df[:30] # 기사 30개만 샘플로\n",
        "print(df1.shape)\n",
        "df1[:3] # 그중 3개만 보여주기"
      ],
      "metadata": {
        "id": "mJj4aaICPQ5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) 기사를 문장별로 분리하기. - 파일을 보고 어떻게 나누는게 좋을지 고민하기"
      ],
      "metadata": {
        "id": "56OrZPDNPV1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 기사를 문장별로 분리하는 사용자 함수\n",
        "separators = ['\\n', '.', '!', '?'] # 엔터 및 문장 종결 부호\n",
        "\n",
        "def split_content_by_separators(content, separators):\n",
        "    \"\"\"Splits content based on the given separators.\"\"\"\n",
        "    escaped_separators = [re.escape(sep) for sep in separators]  # separators = 구분자들\n",
        "    return re.split('|'.join(escaped_separators), content)"
      ],
      "metadata": {
        "id": "Diugf21SPVQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용자 함수 적용하여 문장별로 분리하여 행으로 재구성\n",
        "\n",
        "new_df = df1.apply(lambda row: pd.Series(split_content_by_separators(row['Content'], separators)), axis=1) \\\n",
        "            .stack() \\\n",
        "            .reset_index(level=1, drop=True) \\\n",
        "            .to_frame(name='Content') \\\n",
        "            .join(df1[['자료ID']], how='left')\n",
        "\n",
        "new_df = new_df[['자료ID', 'Content']]\n",
        "print(new_df.shape)\n",
        "new_df"
      ],
      "metadata": {
        "id": "Fy6l4IGzPfgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장별 id 생성 ##### 중요 !!\n",
        "new_sent_df = new_df.reset_index(drop=True)\n",
        "new_sent_df['sent_id'] = new_sent_df.index + 1\n",
        "new_sent_df = new_sent_df[['sent_id', '자료ID', 'Content']]\n",
        "new_sent_df\n",
        "# 320개의 문장이 있다는것을 알 수 있음"
      ],
      "metadata": {
        "id": "ygv7vqBpPhZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) 형태소 분석\n",
        "- 제목: 형태소 분석기 적용\n",
        "- kiwi 형태소 분석기로 텍스트 데이터 분석, 각 언어 별로 형태소 분석기가 있음 - 분석기별 코드는 chatCPT로 만들어보자.\n",
        "- 한국어 형태소 분석기(kiwi)에서 제공하는 서브워드(subword) 학습 함수를 활용해 1920~40년에 출간된 조선/동아일보 약 190만건의 기사를 학습 후 만든 근대 한국어 토크나이저.(카이스트 김병준) https://github.com/ByungjunKim/ModernKoreanSubword\n",
        "- cf) 유튜브 동영상 https://www.youtube.com/watch?v=5VmKXYwnYeQ"
      ],
      "metadata": {
        "id": "1FmP5tZiPjsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# kiwi 설치\n",
        "!pip install -U kiwipiepy"
      ],
      "metadata": {
        "id": "-H3gyuQMPwjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 서브워드 토크나이저 json 파일 다운로드\n",
        "!git clone https://github.com/ByungjunKim/ModernKoreanSubword"
      ],
      "metadata": {
        "id": "IsFaqu0YPzGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# kiwi 로드\n",
        "from kiwipiepy import Kiwi\n",
        "kiwi = Kiwi()\n",
        "from kiwipiepy.sw_tokenizer import SwTokenizer\n",
        "\n",
        "# 토크나이저 설정\n",
        "tokenizer = SwTokenizer('./ModernKoreanSubword/ModernKoreanSubword.json', kiwi=kiwi)"
      ],
      "metadata": {
        "id": "d3cEz67jP0Y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1음절 한자 이어질 때 두 개를 합해 줌 - 아직 한국 한문의 형태소를 분석해주는 분석기는 없음. 따라서 나만의 기준으로 한자 개념어를 추출해야 함.\n",
        "# 여기서는 간단히 한자를 하나씩 뽑아내서 2개씩 합치는 형태로 진행.\n",
        "\n",
        "def is_hanja(ch):\n",
        "    # 한자의 범위는 U+4E00 부터 U+9FFF 입니다.\n",
        "    return '\\u4e00' <= ch <= '\\u9fff'\n",
        "\n",
        "def combine_one_hanja(lst):\n",
        "    result = []\n",
        "    i = 0\n",
        "    while i < len(lst):\n",
        "        if (len(lst[i]) == 1 and i + 1 < len(lst) and len(lst[i + 1]) == 1 and\n",
        "        is_hanja(lst[i]) and is_hanja(lst[i + 1])):\n",
        "            result.append(lst[i] + lst[i + 1])\n",
        "            i += 2\n",
        "        else:\n",
        "            result.append(lst[i])\n",
        "            i += 1\n",
        "    return result"
      ],
      "metadata": {
        "id": "QcLJS5u3P2V9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 영어, 숫자, 특정 부호 등 제외하기\n",
        "\n",
        "def filter_elements(lst):\n",
        "    # 영어, 숫자 및 '<' 문자만을 허용하는 정규식 패턴\n",
        "    pattern = re.compile(r'^[a-zA-Z0-9<＞「」]+$')\n",
        "\n",
        "    # 해당 패턴과 일치하지 않는 문자열만을 반환하는 필터링 함수\n",
        "    return list(filter(lambda x: not pattern.match(x), lst))"
      ],
      "metadata": {
        "id": "E5TD92B6P-cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 본격 형태소 분석\n",
        "\n",
        "import string\n",
        "def do_morph(sent):\n",
        "    # 토큰나이징 결과 확인 (subword + 한국어 형태소)\n",
        "    vocab_list = tokenizer.encode(sent)\n",
        "    result01 = [tokenizer.id2vocab[i].replace('#', '') for i in vocab_list if '/' not in tokenizer.id2vocab[i]]\n",
        "    result01 = [''.join([char for char in word if char not in string.punctuation]) for word in result01]\n",
        "    result01 = [word for word in result01 if len(word) > 0] # 공백 제외\n",
        "    result01 = combine_one_hanja(result01) # 연속되는 1음절 한자 합침 (사용자 함수)\n",
        "    result01 = filter_elements(result01) # 영어, 숫자, 특정부호 제외 (사용자 함수)\n",
        "\n",
        "    return result01"
      ],
      "metadata": {
        "id": "bNZ_jslgQS6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 자료 확인해보기\n",
        "new_sent_df[:3]"
      ],
      "metadata": {
        "id": "KXlO2OyvQanD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'Content' 열에 kiwi 형태소 분석기 적용\n",
        "new_sent_df['tokens'] = new_sent_df['Content'].apply(do_morph)\n",
        "new_sent_df"
      ],
      "metadata": {
        "id": "0mEJoPTMQcEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'tokens' 열에서 리스트 요소가 하나도 없는 행 제거\n",
        "new_sent_df1 = new_sent_df[new_sent_df['tokens'].str.len() > 0]\n",
        "\n",
        "new_sent_df1 = new_sent_df1.copy()\n",
        "new_sent_df1['tokens'] = new_sent_df1['tokens'].apply(lambda x: ' '.join(x)) # 리스트를 문자열로\n",
        "\n",
        "new_sent_df1.reset_index(drop=True, inplace=True) # 인덱스 초기화\n",
        "new_sent_df1['sent_id'] = new_sent_df1.index + 1 # sent_id 재넘버링\n",
        "new_sent_df1.head()"
      ],
      "metadata": {
        "id": "gUQOKbW4Qfqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 단어 빈도 분석 및 chatGPT로 연도별 개념어 빈도 분석해보기"
      ],
      "metadata": {
        "id": "60Pc4JvoQlio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 'tokens' 열의 문자열을 빈칸으로 분리하여 모든 단어들을 리스트로 추출\n",
        "words = new_sent_df1['tokens'].str.split().explode()\n",
        "words"
      ],
      "metadata": {
        "id": "xsczLqNYQtvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어들의 빈도 계산\n",
        "word_counts = words.value_counts()\n",
        "\n",
        "# 결과 출력\n",
        "print(word_counts[:20])"
      ],
      "metadata": {
        "id": "PP3Hygc7Qv6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 인덱스(단어)의 길이가 2 이상인 것만 선택\n",
        "word_counts_1 = word_counts[word_counts.index.str.len() >= 2]\n",
        "word_counts_1[:20]"
      ],
      "metadata": {
        "id": "LpKGqwdjQxOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_sent_df1.to_excel('result/A2.xlsx', index=False)  # 인덱스를 저장하지 않도록 설정"
      ],
      "metadata": {
        "id": "uYiubT3tSmfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## result 폴더에서 우리가 만든 파일 두 개를 다운받아 보자. 그리고 chatGPT에 업로드하여 다음과 같이 질문해보자.\n",
        "\n",
        "\"a1은 기사를 스크래핑 한 것이고, a2에는 문장별로 기사의 내용을 나누어서 형태소 분석기로 단어를 추출해놓은 것까지 진행한 것이야. 두 파일을 연동하여 연도별로 2글자 이상의 개념어의 빈도수를 알고 싶어\"\n",
        "\n",
        "\"그리고 각 연도별 가장 많이 나온 단어 top3를 알려줘\""
      ],
      "metadata": {
        "id": "hEjhc-tnUqVm"
      }
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "toc-autonumbering": false
  },
  "nbformat": 4,
  "nbformat_minor": 0
}